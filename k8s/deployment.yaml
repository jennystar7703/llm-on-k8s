apiVersion: apps/v1
kind: Deployment
metadata:
  name: deepseek-r1-server
  labels:
    app: deepseek-r1
spec:
  replicas: 2
  selector:
    matchLabels:
      app: deepseek-r1
  template:
    metadata:
      labels:
        app: deepseek-r1
    spec:
      restartPolicy: Always
      #runtimeClassName: nvidia
      containers:
      - name: vllm-container
        image: vllm-openai-server
        env: # <-- ADD ENVIRONMENT VARIABLES HERE
          - name: NCCL_DEBUG
            value: "INFO"
          - name: NCCL_P2P_DISABLE # 0 for enabled, 1 for disabled
            value: "0"
          - name: VLLM_WORKER_MULTIPROC_METHOD
            value: "spawn"
          - name: PYTORCH_CUDA_ALLOC_CONF
            value: "expandable_segments:True"
            # CUDA_VISIBLE_DEVICES is managed by Kubernetes via nvidia.com/gpu resource requests
        command: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
        args:
          - "--model"
          - "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
          - "--quantization"
          - "awq"
          - "--dtype"
          - "auto"
          - "--tensor-parallel-size"
          - "4" # fit on 4 GPUs with TP=4
          - "--gpu-memory-utilization"
          - "0.95" # Use 95% of GPU memory
          - "--max-model-len"
          - "8192" # Adjust based on model's capabilities
          - "--port"
          - "8000"
          - "--host"
          - "0.0.0.0"
          # - "--disable-log-requests" # Optional: for less verbose logging
        ports:
        - name: http
          containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: "4" # Request 4 GPUs
          requests:
            nvidia.com/gpu: "4" # Must match limits
        livenessProbe:
          httpGet:
            path: /health # vLLM's OpenAI endpoint has /health
            port: http
          initialDelaySeconds: 600 # Generous time for model download & loading
          periodSeconds: 30
          timeoutSeconds: 15
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 600
          periodSeconds: 30
          timeoutSeconds: 15
          successThreshold: 1
        volumeMounts:
          - name: model-cache
            mountPath: /home/filadmin/.cache/huggingface # Default HF cache
          - name: dshm
            mountPath: /dev/shm
      volumes:
        - name: model-cache
          hostPath:
            path: /home/filadmin/ai/hf_cache_deepseek_r1 # Create this dir on your K8s node
            type: DirectoryOrCreate
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 32Gi # Adjust based on vLLM needs (Ray)
      terminationGracePeriodSeconds: 120